{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datamodel import DataModel\n",
    "\n",
    "#user evaluation\n",
    "json_file_path = '/asdf.json'\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_model = DataModel.parse_obj(data)\n",
    "\n",
    "df = data_model.tabular_df()\n",
    "df.insert(0, 'id', range(1, 1 + len(df)))\n",
    "data_cols = ['metrics_pass','user_pass',\n",
    "# 'similarity',\n",
    "'context-relevance',\n",
    "'context-faithfulness',\n",
    "'Factuality',\n",
    "'Tone',\n",
    "'answer-relevance']\n",
    "\n",
    "THRESHOLD = 0.6\n",
    "\n",
    "# add a calculated score column\n",
    "metrics_cols = [\n",
    "    # 'similarity',\n",
    "    'context-relevance',\n",
    "    'context-faithfulness',\n",
    "    'Factuality',\n",
    "    'Tone',\n",
    "    'answer-relevance']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and THRESHOLD is defined\n",
    "THRESHOLD = 0.6  # Example threshold, adjust as needed\n",
    "\n",
    "# Define the calculation as a function\n",
    "def calculate_score(row):\n",
    "    calced_score = 1  # Start assuming the score is 1\n",
    "    for col in metrics_cols:\n",
    "        # Check if the column value exists and is below the threshold\n",
    "        if pd.notnull(row[col]) and row[col] < THRESHOLD:\n",
    "            calced_score = 0\n",
    "            break  # Exit loop early if any score is below threshold\n",
    "    return calced_score\n",
    "\n",
    "# Apply the function to each row\n",
    "df['metrics_pass'] = df.apply(calculate_score, axis=1)\n",
    "\n",
    "# rename column\n",
    "df['user_pass'] = df['pass_field']\n",
    "df = df.drop(columns=['pass_field'])\n",
    "\n",
    "df['context_len'] = df['context'].apply(lambda x: len(str(x)))\n",
    "df['context_len']\n",
    "\n",
    "# rename column\n",
    "df['ground_truth'] = df['value']\n",
    "df = df.drop(columns=['value'])\n",
    "\n",
    "# reorder the df\n",
    "df = df[['id','query','text','ground_truth','context','comment','Tone','context-relevance','answer-relevance','context-faithfulness','Factuality',\n",
    "        #  'similarity',\n",
    "         'metrics_pass','user_pass','context_len']]\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all rows where context relevance is 0\n",
    "df[df['context-relevance'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['user_pass'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create a confusion matrix\n",
    "\n",
    "y_true = df['user_pass']\n",
    "y_pred = df['context-relevance'] > 0\n",
    "cm = confusion_matrix(y_pred, y_true)\n",
    "\n",
    "#plot cm\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel('Predicted - Promptfoo')\n",
    "plt.ylabel('Actual - User evaluation')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all false negatives \n",
    "df[(df['user_pass'] == 1) & (df['context-relevance'] == 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has been properly defined with 'user_pass' and 'context-relevance' columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a confusion matrix\n",
    "y_true = df['user_pass']\n",
    "y_pred = df['context-relevance'] > 0\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Adjust to appropriate scale for your paper\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False)  # 'Blues' or 'Greys' for monochromatic color\n",
    "plt.xlabel('Predicted - Promptfoo', fontsize=14)\n",
    "plt.ylabel('Actual - User Evaluation', fontsize=14)\n",
    "plt.title('Confusion Matrix of Context Relevance Prediction', fontsize=16)\n",
    "plt.xticks([0.5, 1.5], ['Negative', 'Positive'])  # Assuming binary classification: 0-Negative, 1-Positive\n",
    "plt.yticks([0.5, 1.5], ['Negative', 'Positive'], va='center')\n",
    "plt.tight_layout()  # Adjust layout to make room for label\n",
    "plt.show()\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# f1 \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do an ROC \n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "y_true = df['user_pass']\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, df['context-relevance'])\n",
    "roc_auc = roc_auc_score(y_true, df['context-relevance'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for context-relevance')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# calcualate yudens index\n",
    "yudens = tpr - fpr\n",
    "optimal_idx = np.argmax(yudens)\n",
    "print(f'Optimal Threshold: {thresholds[optimal_idx]}')\n",
    "\n",
    "# Create a confusion matrix\n",
    "y_true = df['user_pass']\n",
    "\n",
    "# use the threshold to predict\n",
    "y_pred = [1 if e >= thresholds[optimal_idx] else 0 for e in df['context-relevance']]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Adjust to appropriate scale for your paper\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False)  # 'Blues' or 'Greys' for monochromatic color\n",
    "plt.xlabel('Predicted - Promptfoo', fontsize=14)\n",
    "plt.ylabel('Actual - User Evaluation', fontsize=14)\n",
    "plt.title('Confusion Matrix of context-relevance Prediction', fontsize=16)\n",
    "plt.xticks([0.5, 1.5], ['Negative', 'Positive'])  # Assuming binary classification: 0-Negative, 1-Positive\n",
    "plt.yticks([0.5, 1.5], ['Negative', 'Positive'], va='center')\n",
    "plt.tight_layout()  # Adjust layout to make room for label\n",
    "plt.show()\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# f1 \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do an ROC \n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "y_true = df['user_pass']\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, df['context-faithfulness'])\n",
    "roc_auc = roc_auc_score(y_true, df['context-faithfulness'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for context-faithfulness')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# calcualate yudens index\n",
    "yudens = tpr - fpr\n",
    "optimal_idx = np.argmax(yudens)\n",
    "print(f'Optimal Threshold: {thresholds[optimal_idx]}')\n",
    "\n",
    "# Create a confusion matrix\n",
    "y_true = df['user_pass']\n",
    "\n",
    "# use the threshold to predict\n",
    "y_pred = [1 if e >= thresholds[optimal_idx] else 0 for e in df['context-faithfulness']]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Adjust to appropriate scale for your paper\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False)  # 'Blues' or 'Greys' for monochromatic color\n",
    "plt.xlabel('Predicted - Promptfoo', fontsize=14)\n",
    "plt.ylabel('Actual - User Evaluation', fontsize=14)\n",
    "plt.title('Confusion Matrix of context-faithfulness Prediction', fontsize=16)\n",
    "plt.xticks([0.5, 1.5], ['Negative', 'Positive'])  # Assuming binary classification: 0-Negative, 1-Positive\n",
    "plt.yticks([0.5, 1.5], ['Negative', 'Positive'], va='center')\n",
    "plt.tight_layout()  # Adjust layout to make room for label\n",
    "plt.show()\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# f1 \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of distribution plots for each column\n",
    "import seaborn as sns\n",
    "\n",
    "# create a matrix of distribution plots for each column\n",
    "# sns.pairplot(df[[\n",
    "#     # 'similarity',\n",
    "#     'context-relevance','context-faithfulness', 'answer-relevance']])\n",
    "\n",
    "# create a distribution plot for each column \n",
    "sns.displot(df['context-relevance'], kde=True)\n",
    "sns.displot(df['context-faithfulness'], kde=True)\n",
    "sns.displot(df['answer-relevance'], kde=True)\n",
    "\n",
    "# summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all cols where context faithfulness is !=1\n",
    "df[df['context-faithfulness'] != 1]\n",
    "\n",
    "# count how many rows have context faithfulness != 1\n",
    "print(f\"Number of rows where context relevance is not 1: {len(df[df['context-relevance'] != 1])}\")\n",
    "print(f\"contex_relevance accuracy: {round(1-len(df[df['context-relevance'] != 1])/len(df['context-relevance']),2)}\")\n",
    "print(f\"Number of rows where context faithfulness is not 1: {len(df[df['context-faithfulness'] != 1])}\")\n",
    "print(f\"context-faithfulness accuracy: {round(1-len(df[df['context-faithfulness'] != 1])/len(df['context-faithfulness']),2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of distribution plots for each column\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a matrix of distribution plots for each column\n",
    "# treat the context-faithfulness column as a binary indicator\n",
    "df['context-faithfulness_binary'] = df['context-faithfulness'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# sns.pairplot(df[['context-relevance','context-faithfulness_binary','user_pass']])\n",
    "\n",
    "# create a confusion matrix for context faithfulness binary and user pass \n",
    "y_true = df['user_pass']\n",
    "y_pred = df['context-faithfulness_binary']\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Adjust to appropriate scale for your paper\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False)  # 'Blues' or 'Greys' for monochromatic color\n",
    "plt.xlabel('Predicted - Promptfoo', fontsize=14)\n",
    "plt.ylabel('Actual - User Evaluation', fontsize=14)\n",
    "plt.title('Confusion Matrix of Context Faithfulness Prediction', fontsize=16)\n",
    "plt.xticks([0.5, 1.5], ['Negative', 'Positive'])  # Assuming binary classification: 0-Negative, 1-Positive\n",
    "plt.yticks([0.5, 1.5], ['Negative', 'Positive'], va='center')\n",
    "plt.tight_layout()  # Adjust layout to make room for label\n",
    "plt.show()\n",
    "\n",
    "# calculate the f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_true, y_pred))\n",
    "#print accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each column and calculate the average, excluding nan values\n",
    "colnames = []\n",
    "averages = []\n",
    "for col in data_cols:\n",
    "    avg = round(df[col].mean(),2)\n",
    "    averages.append(avg)\n",
    "    colnames.append(col)\n",
    "    print(f'Average {col}: {avg}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.bar(colnames, averages)\n",
    "plt.xlabel('Column Name')\n",
    "plt.ylabel('Average')\n",
    "plt.title('Average by Column')\n",
    "\n",
    "# add a line at y=0.6 going through the whole graph\n",
    "plt.axhline(y=0.6, color='r', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each column and calculate the average, excluding nan values\n",
    "colnames = []\n",
    "averages = []\n",
    "df_user_failures = df[df['user_pass'] == False]\n",
    "for col in data_cols:\n",
    "    avg = round(df_user_failures[col].mean(),2)\n",
    "    averages.append(avg)\n",
    "    colnames.append(col)\n",
    "    print(f'Average {col}: {avg}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.bar(colnames, averages)\n",
    "plt.xlabel('Column Name')\n",
    "plt.ylabel('Average')\n",
    "plt.title('Average by Column for failing questions due to user evaluation')\n",
    "\n",
    "# add a line at y=0.6 going through the whole graph\n",
    "plt.axhline(y=0.6, color='r', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['user_pass'] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_failures = df[df['metrics_pass'] == 1]\n",
    "\n",
    "df_different = df_metric_failures[df_metric_failures['user_pass'] == False]\n",
    "\n",
    "df_different\n",
    "# df_metric_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each column and calculate the average, excluding nan values\n",
    "colnames = []\n",
    "averages = []\n",
    "df_metric_failures = df[df['metrics_pass'] == 0]\n",
    "for col in data_cols:\n",
    "    avg = round(df_metric_failures[col].mean(),2)\n",
    "    averages.append(avg)\n",
    "    colnames.append(col)\n",
    "    print(f'Average {col}: {avg}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.bar(colnames, averages)\n",
    "plt.xlabel('Column Name')\n",
    "plt.ylabel('Average')\n",
    "plt.title('Average by Column for failing questions due to metrics evaluation')\n",
    "\n",
    "# add a line at y=0.6 going through the whole graph\n",
    "plt.axhline(y=0.6, color='r', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metrics_pass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create a confusion matrix\n",
    "\n",
    "y_true = df['user_pass']\n",
    "y_pred = df['metrics_pass'] > 0\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#plot cm\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel('Predicted - Promptfoo')\n",
    "plt.ylabel('Actual - User evaluation')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df_user_failures = df[df['user_pass'] == False]\n",
    "df_metric_failures = df[df['metrics_pass'] == 0]\n",
    "#make a boxplot, make sure there is space for the labels\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.title('Boxplot of explainers for failing questions due to user eval')\n",
    "#plot the boxplot, excluding the id column and outliers\n",
    "sns.boxplot(data=df_user_failures[data_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dfs =df[data_cols].mean()\n",
    "mean_dfs.plot(kind='bar', x='query', y=['similarity','context-relevance','context-faithfulness','Factuality','Tone','answer-relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similarity and index value as a bar plot\n",
    "df.plot(kind='bar', x='query', y=[\n",
    "    # 'similarity',\n",
    "    'context-relevance','context-faithfulness','Factuality','Tone','answer-relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a heatmap of the df\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.heatmap(df[data_cols].corr(), annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of distribution plots for each column\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a matrix of distribution plots for each column\n",
    "sns.pairplot(df[[\n",
    "    # 'similarity',\n",
    "    'context-relevance','context-faithfulness','Factuality','Tone', 'answer-relevance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of distribution plots for each column\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a matrix of distribution plots for each column\n",
    "sns.pairplot(df_user_failures[data_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to csv \n",
    "df.to_csv('/asdf.csv', index=False,sep=\";\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['user_pass'] == False]\n",
    "\n",
    "df_user_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
